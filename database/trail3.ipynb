{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user google-cloud-aiplatform>=1.29.0 google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ae82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from google.cloud import storage\n",
    "import uuid\n",
    "from langchain_google_vertexai import VectorSearchVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from Multi_Modal.chunking import get_chunks\n",
    "from Multi_Modal.SeperationAndSummarization import summarize_chunks\n",
    "from conversion import convert_to_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "REGION = os.environ['REGION']\n",
    "BUCKET_NAME=os.environ['BUCKET_NAME']\n",
    "INDEX_DISPLAY_NAME=os.environ['INDEX_DISPLAY_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18338e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f27d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID,location=REGION)\n",
    "\n",
    "embeddings= GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_vector_to_gcs(documents, bucket_name=BUCKET_NAME):\n",
    "\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    print(\"Completed Login\")\n",
    "\n",
    "    texts = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        try:\n",
    "            raw_text = \"\"\n",
    "            if hasattr(doc, \"metadata\") and \"original_content\" in doc.metadata:\n",
    "                content_str = doc.metadata.get(\"original_content\")\n",
    "                if content_str:\n",
    "                    data = json.loads(content_str)\n",
    "                    raw_text = data.get('raw_text', \"\")\n",
    "            \n",
    "            if not raw_text:\n",
    "                raw_text = doc.page_content \n",
    "\n",
    "            texts.append(raw_text)\n",
    "            valid_indices.append(i)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing doc {i}: {e}\")\n",
    "            texts.append(\"\") \n",
    "\n",
    "    print(f\"Generating embeddings for {len(texts)} documents...\")\n",
    "    vectors = embeddings.embed_documents(texts)\n",
    "    if vectors:\n",
    "        print(f\"!!! ACTUAL VECTOR DIMENSION: {len(vectors[0])} !!!\") \n",
    "\n",
    "\n",
    "    vertex_content = []\n",
    "    \n",
    "    for i, doc_index in enumerate(valid_indices):\n",
    "        doc = documents[doc_index]\n",
    "        \n",
    "        docs_id = str(uuid.uuid4()) \n",
    "        \n",
    "        metadata_blob = bucket.blob(f\"docstore/{docs_id}.json\")\n",
    "        \n",
    "        try:\n",
    "            content = json.loads(doc.metadata.get(\"original_content\", \"{}\"))\n",
    "        except:\n",
    "            content = {}\n",
    "\n",
    "        bucket_content = {\n",
    "            'id': docs_id,\n",
    "            'raw_text': content.get('raw_text', texts[i]), \n",
    "            'table_as_html': content.get('table_html', []),\n",
    "            'image_base64': content.get('image_base64', [])\n",
    "        }\n",
    "        \n",
    "        metadata_blob.upload_from_string(json.dumps(bucket_content))\n",
    "\n",
    "        vertex_record = {\n",
    "            \"id\": docs_id,\n",
    "            \"embedding\": vectors[i] \n",
    "        }\n",
    "        vertex_content.append(json.dumps(vertex_record))\n",
    "\n",
    "    vector_data = \"\\n\".join(vertex_content)\n",
    "    \n",
    "    unique_folder = f\"init_vectors_{uuid.uuid4()}\"\n",
    "    \n",
    "    blob_name = f\"{unique_folder}/vectors.json\"\n",
    "    vector_blob = bucket.blob(blob_name)\n",
    "    vector_blob.upload_from_string(vector_data)\n",
    "\n",
    "    print(f\"Success! Metadata in gs://{bucket_name}/docstore/\")\n",
    "    print(f\"Vectors ready in gs://{bucket_name}/{unique_folder}/\")\n",
    "    \n",
    "    gcs_uri = f\"gs://{bucket_name}/{unique_folder}/\"\n",
    "    return gcs_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea98f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "number=\"init_vector_8be84b8c-5da2-4b61-b02e-885a53cbb735\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d29f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_url = f\"gs://{BUCKET_NAME}/{number}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f13098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_deploy_index(gcs_uri):\n",
    "\n",
    "    import traceback\n",
    "    print(\"Creating Index (takes time)...\")\n",
    "\n",
    "    dimensions = 3072\n",
    "    approximate_neighbors_count = 150\n",
    "    leaf_node_embedding_count = 100\n",
    "    leaf_nodes_to_search_percent = 10\n",
    "    distance_measure_type = \"DOT_PRODUCT_DISTANCE\"\n",
    "\n",
    "    print(\"Index params:\", {\n",
    "        'dimensions': dimensions,\n",
    "        'approximate_neighbors_count': approximate_neighbors_count,\n",
    "        'leaf_node_embedding_count': leaf_node_embedding_count,\n",
    "        'leaf_nodes_to_search_percent': leaf_nodes_to_search_percent,\n",
    "        'distance_measure_type': distance_measure_type,\n",
    "        'contents_delta_uri': gcs_uri,\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "            display_name=INDEX_DISPLAY_NAME,\n",
    "            contents_delta_uri=gcs_uri,\n",
    "            dimensions=dimensions,\n",
    "            approximate_neighbors_count=approximate_neighbors_count,\n",
    "            leaf_node_embedding_count=leaf_node_embedding_count,\n",
    "            leaf_nodes_to_search_percent=leaf_nodes_to_search_percent,\n",
    "            distance_measure_type=distance_measure_type,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Index creation failed. Exception repr:\", repr(e))\n",
    "        try:\n",
    "            print(\"Exception type:\", type(e))\n",
    "            if hasattr(e, 'errors'):\n",
    "                print(\"e.errors:\", e.errors)\n",
    "            if hasattr(e, 'message'):\n",
    "                print(\"e.message:\", e.message)\n",
    "        except Exception as diag_exc:\n",
    "            print(\"Error printing exception attributes:\", diag_exc)\n",
    "        print(\"Full traceback:\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"Creating Endpoint\")\n",
    "    try:\n",
    "        my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "            display_name=f\"{INDEX_DISPLAY_NAME}_endpoint\",\n",
    "            public_endpoint_enabled=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Endpoint creation failed:\", repr(e))\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"Deploying Index to Endpoint\")\n",
    "    try:\n",
    "        my_index_endpoint.deploy_index(\n",
    "            index=my_index,\n",
    "            deployed_index_id=\"soumya_deployed_v1\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Deploy failed:\", repr(e))\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"deployment Completed\")\n",
    "    return my_index_endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vertex_ai(query,index_endpoint, deployed_index_id, bucket_name=BUCKET_NAME):\n",
    "\n",
    "    query_emb = embeddings.embed_query(query)\n",
    "\n",
    "    response = index_endpoint.find_neighbors(\n",
    "        deployed_index_id=deployed_index_id,\n",
    "        queries = [query_emb],\n",
    "        num_neighbors=5\n",
    "    )\n",
    "\n",
    "    print(\"Response:\",response)\n",
    "\n",
    "\n",
    "    results=[]\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    print(\"Login successed\")\n",
    "\n",
    "    for neighbor in response[0]:\n",
    "        doc_id = neighbor.id\n",
    "        score = neighbor.distance\n",
    "\n",
    "        blob = bucket.blob(f\"docstore/{doc_id}.json\")\n",
    "        if blob.exists():\n",
    "            data = json.loads(blob.download_as_text())\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=data.get('raw_text', \"\"), \n",
    "                metadata={\n",
    "                    \"tables\": data.get('table_as_html', []),\n",
    "                    \"images\": data.get('image_base64', []), \n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "        results.append(doc)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d58b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pdfs/Documentation-Project.pdf\"\n",
    "output_dir = \"temp_uploads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir= Path(output_dir)\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"Explain the workflow of the project. explain the tech stack used to build the project and tell how the query is procceed when a query is given to the final model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Chunks\")\n",
    "chunks = get_chunks(file_path)\n",
    "print(\"Chunks Created\")\n",
    "\n",
    "print(\"summarize_chunks\")\n",
    "docs = summarize_chunks(chunks)\n",
    "print(\"Summarization Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f182764",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1].metadata.get('original_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee376c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"upload to GCS\")\n",
    "gcs_uri= upload_vector_to_gcs(docs)\n",
    "print(\"Uploaded Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint= create_and_deploy_index(gcs_uri)\n",
    "print('Completed deploying index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = aiplatform.MatchingEngineIndexEndpoint.list()\n",
    "for ep in endpoints:\n",
    "    print(f\"Name: {ep.display_name}\")\n",
    "    print(f\"ID: {ep.name}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "    index_endpoint_name=f\"projects/{os.environ['PROJECT_NO']}/locations/{REGION}/indexEndpoints/{os.environ['ENDPOINT_ID']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_vertex_ai(\n",
    "    query=query, \n",
    "    index_endpoint=my_endpoint,      \n",
    "    deployed_index_id=\"soumya_deployed_v1\", \n",
    "    bucket_name=BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfde54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819feea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(results[0],\"page_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508273ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_final_ans(results,query):\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(model='gemini-2.5-pro', temperature=0.1)\n",
    "\n",
    "        prompt_text = f\"\"\"\n",
    "        Based on the following document context, please answer this question: {query}\n",
    "        \n",
    "        CONTENT_TO_ANALYZE:\n",
    "        \"\"\"\n",
    "        all_images_base64 = []\n",
    "        for i, chunk in enumerate(results):\n",
    "            prompt_text += f\"\\n--- Document Fragment {i+1} ---\\n\"\n",
    "            if hasattr(chunk,\"page_content\"):\n",
    "                raw_text = chunk.page_content\n",
    "                if raw_text:\n",
    "                    prompt_text += f\"Text:\\n{raw_text}\\n\\n\"\n",
    "            \n",
    "            if hasattr(chunk,\"metdata\"):\n",
    "                table= chunk.metadata.get(\"tables\",[])\n",
    "                if table:\n",
    "                    for j , cnt in enumerate(table):\n",
    "                        prompt_text += f\"Table {j+1}:\\n{cnt}\\n\\n\"\n",
    "                imgs = chunk.metadata.get(\"images\",[])\n",
    "                if imgs:\n",
    "                    all_images_base64.extend(imgs)\n",
    "            \n",
    "        prompt_text += \"\"\" \n",
    "            INSTRUCTIONS:\n",
    "            Provide a clear, comprehensive answer using the text, tables, and images provided above. \n",
    "            If the documents don't contain sufficient information to answer the question, state: \"I don't have enough information to answer the question.\"\n",
    "            \n",
    "            ANSWER:\"\"\"\n",
    "        message_content = [{'type': 'text', 'text': prompt_text}]\n",
    "\n",
    "        for img_b64 in all_images_base64:\n",
    "            message_content.append({\"type\": \"image_url\",\"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}})\n",
    "\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer gen failed: {e}\")\n",
    "        return 'Sorry, I encountered an error generating the answer.'\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_final_ans(results,query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Based on the document context provided, here is an explanation of the project\\'s workflow, its technology stack, and how a query is processed.\\n\\n### Project Workflow\\n\\nThe project operates as a decoupled microservice architecture with distinct frontend and backend services. The end-to-end workflow is as follows:\\n\\n1.  **User Input**: A user interacts with the client application (frontend) by entering a news article and submitting it for classification.\\n2.  **API Request**: The frontend application sends the article text via an asynchronous HTTP request to the exposed backend API endpoint, which is built with FastAPI.\\n3.  **Backend Processing**: The backend receives the text and initiates a two-part verification process:\\n    *   **Model Prediction**: The text is first classified by the fine-tuned DeBERTa model, which returns an initial prediction (\"Fake\" or \"True\").\\n    *   **External Validation**: Simultaneously, the application makes an external API call to a live information source (like a news archive or fact-checking service) to gather real-time context.\\n4.  **Cross-Validation**: The information from the external source is used to cross-check and validate the DeBERTa model\\'s initial prediction, enhancing the system\\'s temporal robustness.\\n5.  **Response**: The backend sends the final, validated classification result back to the frontend.\\n6.  **Result Display**: The frontend receives the binary result (\"Fake\" or \"True\") and renders it to the user in a clear format.\\n\\n### Technology Stack\\n\\nThe project utilizes a modern technology stack for its frontend, backend, machine learning model, and deployment.\\n\\n**Frontend:**\\n*   **Build Tool**: **Vite.js** was used for fast development and optimized bundling.\\n*   **Language**: **TypeScript** was implemented for strong type safety.\\n*   **Styling**: **Tailwind CSS** was used for its utility-first approach to rapid and responsive design.\\n\\n**Backend:**\\n*   **API Framework**: **FastAPI** was chosen for its high performance and asynchronous capabilities, making it ideal for serving the ML model.\\n\\n**Machine Learning:**\\n*   **Core Model**: **microsoft/deberta-v3-base**, a Transformer-based model from Hugging Face.\\n*   **ML Libraries**: **PyTorch** and **Hugging Face Transformers** were used for building, training, and managing the model.\\n*   **Training Management**: The **Hugging Face Trainer API** was used to simplify the fine-tuning process.\\n\\n**Deployment & Infrastructure:**\\n*   **Containerization**: **Docker** was used to package the frontend and backend applications into separate, reproducible containers.\\n*   **Cloud Platform**: **Google Cloud Platform (GCP)** was used to host and serve both the containerized frontend and backend services, ensuring high availability and scalability.\\n\\n### Query Processing by the Final Model\\n\\nWhen a user submits a query (a news article), the backend system processes it through a hybrid approach designed to ensure accuracy and temporal relevance:\\n\\n1.  **Initial Classification**: The raw text from the query is first fed into the fine-tuned **DeBERTa classification model**. This model analyzes the linguistic patterns in the text based on its training and produces a primary prediction of \"Fake\" or \"True\".\\n2.  **External Cross-Validation**: To overcome the limitation of the model\\'s knowledge being restricted to its training data (up to 2019), the system simultaneously makes an **external API call** to a live service like a news archive or a fact-checking explorer.\\n3.  **Verdict Augmentation**: The information retrieved from this external source—such as source veracity or previously debunked claims—is used to **cross-check and validate** the DeBERTa model\\'s prediction.\\n4.  **Final Verdict**: This hybrid process ensures the final verdict is not solely reliant on past linguistic patterns but is also enhanced with real-time, contextual evidence, thereby improving the overall trustworthiness of the classification.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
