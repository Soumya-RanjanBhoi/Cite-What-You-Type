{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417a0c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --user google-cloud-aiplatform>=1.29.0 google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c22f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5b54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749ae82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Cite-What-You-Type\\botenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from google.cloud import storage\n",
    "import uuid\n",
    "from langchain_google_vertexai import VectorSearchVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from Multi_Modal.chunking import get_chunks\n",
    "from Multi_Modal.SeperationAndSummarization import summarize_chunks\n",
    "from conversion import convert_to_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b1727f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615f3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "REGION = os.environ['REGION']\n",
    "BUCKET_NAME=os.environ['BUCKET_NAME']\n",
    "INDEX_DISPLAY_NAME=os.environ['INDEX_DISPLAY_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d431c130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vecor_embedding002'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18338e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f27d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID,location=REGION)\n",
    "\n",
    "embeddings= GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7098b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_vector_to_gcs(documents, bucket_name=BUCKET_NAME):\n",
    "\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    print(\"Completed Login\")\n",
    "\n",
    "    texts = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        try:\n",
    "            raw_text = \"\"\n",
    "            if hasattr(doc, \"metadata\") and \"original_content\" in doc.metadata:\n",
    "                content_str = doc.metadata.get(\"original_content\")\n",
    "                if content_str:\n",
    "                    data = json.loads(content_str)\n",
    "                    raw_text = data.get('raw_text', \"\")\n",
    "            \n",
    "            if not raw_text:\n",
    "                raw_text = doc.page_content \n",
    "\n",
    "            texts.append(raw_text)\n",
    "            valid_indices.append(i)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing doc {i}: {e}\")\n",
    "            texts.append(\"\") \n",
    "\n",
    "    print(f\"Generating embeddings for {len(texts)} documents...\")\n",
    "    vectors = embeddings.embed_documents(texts)\n",
    "    if vectors:\n",
    "        print(f\"!!! ACTUAL VECTOR DIMENSION: {len(vectors[0])} !!!\") \n",
    "\n",
    "\n",
    "    vertex_content = []\n",
    "    \n",
    "    for i, doc_index in enumerate(valid_indices):\n",
    "        doc = documents[doc_index]\n",
    "        \n",
    "        docs_id = str(uuid.uuid4()) \n",
    "        \n",
    "        metadata_blob = bucket.blob(f\"docstore/{docs_id}.json\")\n",
    "        \n",
    "        try:\n",
    "            content = json.loads(doc.metadata.get(\"original_content\", \"{}\"))\n",
    "        except:\n",
    "            content = {}\n",
    "\n",
    "        bucket_content = {\n",
    "            'id': docs_id,\n",
    "            'raw_text': content.get('raw_text', texts[i]), \n",
    "            'table_as_html': content.get('table_html', []),\n",
    "            'image_base64': content.get('image_base64', [])\n",
    "        }\n",
    "        \n",
    "        metadata_blob.upload_from_string(json.dumps(bucket_content))\n",
    "\n",
    "        vertex_record = {\n",
    "            \"id\": docs_id,\n",
    "            \"embedding\": vectors[i] \n",
    "        }\n",
    "        vertex_content.append(json.dumps(vertex_record))\n",
    "\n",
    "    vector_data = \"\\n\".join(vertex_content)\n",
    "    \n",
    "    unique_folder = f\"init_vectors_{uuid.uuid4()}\"\n",
    "    \n",
    "    blob_name = f\"{unique_folder}/vectors.json\"\n",
    "    vector_blob = bucket.blob(blob_name)\n",
    "    vector_blob.upload_from_string(vector_data)\n",
    "\n",
    "    print(f\"Success! Metadata in gs://{bucket_name}/docstore/\")\n",
    "    print(f\"Vectors ready in gs://{bucket_name}/{unique_folder}/\")\n",
    "    \n",
    "    gcs_uri = f\"gs://{bucket_name}/{unique_folder}/\"\n",
    "    return gcs_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f13098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_deploy_index(gcs_uri):\n",
    "\n",
    "    import traceback\n",
    "    print(\"Creating Index (takes time)...\")\n",
    "\n",
    "    # Parameters (print them so we can debug what is sent to the API)\n",
    "    dimensions = 3072\n",
    "    approximate_neighbors_count = 150\n",
    "    leaf_node_embedding_count = 100\n",
    "    leaf_nodes_to_search_percent = 10\n",
    "    distance_measure_type = \"DOT_PRODUCT_DISTANCE\"\n",
    "\n",
    "    print(\"Index params:\", {\n",
    "        'dimensions': dimensions,\n",
    "        'approximate_neighbors_count': approximate_neighbors_count,\n",
    "        'leaf_node_embedding_count': leaf_node_embedding_count,\n",
    "        'leaf_nodes_to_search_percent': leaf_nodes_to_search_percent,\n",
    "        'distance_measure_type': distance_measure_type,\n",
    "        'contents_delta_uri': gcs_uri,\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "            display_name=INDEX_DISPLAY_NAME,\n",
    "            contents_delta_uri=gcs_uri,\n",
    "            dimensions=dimensions,\n",
    "            approximate_neighbors_count=approximate_neighbors_count,\n",
    "            # Required algorithm config params for Tree AH\n",
    "            leaf_node_embedding_count=leaf_node_embedding_count,\n",
    "            leaf_nodes_to_search_percent=leaf_nodes_to_search_percent,\n",
    "            distance_measure_type=distance_measure_type,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Print rich diagnostics to help debug FAILED_PRECONDITION\n",
    "        print(\"Index creation failed. Exception repr:\", repr(e))\n",
    "        try:\n",
    "            print(\"Exception type:\", type(e))\n",
    "            # some google exceptions carry extra attributes\n",
    "            if hasattr(e, 'errors'):\n",
    "                print(\"e.errors:\", e.errors)\n",
    "            if hasattr(e, 'message'):\n",
    "                print(\"e.message:\", e.message)\n",
    "        except Exception as diag_exc:\n",
    "            print(\"Error printing exception attributes:\", diag_exc)\n",
    "        print(\"Full traceback:\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"Creating Endpoint\")\n",
    "    try:\n",
    "        my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "            display_name=f\"{INDEX_DISPLAY_NAME}_endpoint\",\n",
    "            public_endpoint_enabled=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Endpoint creation failed:\", repr(e))\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"Deploying Index to Endpoint\")\n",
    "    try:\n",
    "        my_index_endpoint.deploy_index(\n",
    "            index=my_index,\n",
    "            deployed_index_id=\"soumya_deployed_v1\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Deploy failed:\", repr(e))\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"deployment Completed\")\n",
    "    return my_index_endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5bf15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vertex_ai(query,index_endpoint, deployed_index_id, bucket_name=BUCKET_NAME):\n",
    "\n",
    "    query_emb = embeddings.embed_query(query)\n",
    "\n",
    "    response = index_endpoint.find_neighbors(\n",
    "        deployed_index_id=deployed_index_id,\n",
    "        queries = [query_emb],\n",
    "        num_neighbors=5\n",
    "    )\n",
    "\n",
    "    print(\"Response:\",response)\n",
    "\n",
    "\n",
    "    results=[]\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    print(\"Login successed\")\n",
    "\n",
    "    for neighbor in response[0]:\n",
    "        doc_id = neighbor.id\n",
    "        score = neighbor.distance\n",
    "\n",
    "        blob = bucket.blob(f\"docstore/{doc_id}.json\")\n",
    "        if blob.exists():\n",
    "            data = json.loads(blob.download_as_text())\n",
    "\n",
    "            # In search_vertex_ai...\n",
    "            doc = Document(\n",
    "                page_content=data.get('raw_text', \"\"), \n",
    "                metadata={\n",
    "                    \"tables\": data.get('table_as_html', []),\n",
    "                    \"images\": data.get('image_base64', []), \n",
    "                    \"score\": score\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "        results.append((doc,score))\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47c1bf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Cite-What-You-Type'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8d58b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pdfs/Documentation-Project.pdf\"\n",
    "output_dir = \"temp_uploads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e5e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7355fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir= Path(output_dir)\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7737a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"Explain the workflow of the project. explain the tech stack used to build the project and tell how the query is procceed when a query is given to the final model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Chunks\")\n",
    "chunks = get_chunks(file_path)\n",
    "print(\"Chunks Created\")\n",
    "\n",
    "print(\"summarize_chunks\")\n",
    "docs = summarize_chunks(chunks)\n",
    "print(\"Summarization Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f182764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"raw_text\": \"Advertisement and Publicity Team\\\\n\\\\n1.Shradhanjali Das (124AI0024) 2.Gangala Tanishka (124AI0005) 3.Suraj Kumar Sahu (124AI0013)\\\\n\\\\nPROBLEM STATEMENT :\\\\n\\\\nTitle: Automated Fake News Detection System Development\\\\n\\\\nIn the modern digital age, the rapid dissemination of unverified and misleading information\\\\u2014commonly referred to as \\\\\"fake news\\\\\" or \\\\\"information disorder\\\\\"\\\\u2014poses a critical threat to democratic processes, public health, and social stability. The sheer volume and velocity of content generated daily across platforms make manual verification impossible, necessitating advanced automated solutions. The ability of malicious actors to use sophisticated techniques to spread hoaxes and disinformation undermines trust in legitimate news sources and contributes to polarization.\\\\n\\\\nThis project focuses on the development of a robust and scalable Fake News Detection (FND) system designed to identify and flag deceptive articles. The system leverages a curated, static training corpus, such as the ISOT dataset (containing news articles up to late 2019 across politics, government, and US news), to learn core patterns of disinformation. For real-time prediction and adaptability to current events, the system integrates live data acquisition through API calls to major news sources. The training data, comprising real and fabricated news articles, encompasses textual content and their ground-truth labels, spanning multiple domains (including political, economic, etc.), allowing for the creation of a generalized detection model.\\\\n\\\\nThe primary goal of this analysis is to design, implement, and evaluate a machine learning pipeline capable of accurately distinguishing between legitimate and deceptive news content in real time.\", \"table_html\": [], \"image_base64\": []}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata.get('original_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee376c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload to GCS\n",
      "Completed Login\n",
      "Generating embeddings for 14 documents...\n",
      "Completed Login\n",
      "Generating embeddings for 14 documents...\n",
      "!!! ACTUAL VECTOR DIMENSION: 3072 !!!\n",
      "!!! ACTUAL VECTOR DIMENSION: 3072 !!!\n",
      "Success! Metadata in gs://vecor_embedding002/docstore/\n",
      "Vectors ready in gs://vecor_embedding002/init_vectors_8be84b8c-5da2-4b61-b02e-885a53cbb735/\n",
      "Uploaded Completed\n",
      "Success! Metadata in gs://vecor_embedding002/docstore/\n",
      "Vectors ready in gs://vecor_embedding002/init_vectors_8be84b8c-5da2-4b61-b02e-885a53cbb735/\n",
      "Uploaded Completed\n"
     ]
    }
   ],
   "source": [
    "print(\"upload to GCS\")\n",
    "gcs_uri= upload_vector_to_gcs(docs)\n",
    "print(\"Uploaded Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7defe5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://vecor_embedding002/init_vectors_8be84b8c-5da2-4b61-b02e-885a53cbb735/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcs_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3bda72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files in gs://embeddings_store001/init_vectors_e5614764-d6d7-47e5-988f-bdda705e9a4a/\n",
      "\n",
      "Checking file: init_vectors_e5614764-d6d7-47e5-988f-bdda705e9a4a/vectors.json (594878 bytes)\n",
      "Total lines: 14\n",
      "First record keys: dict_keys(['id', 'embedding'])\n",
      "Vector dim: 3072\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "# Paste the URI that failed here\n",
    "FAILED_URI = gcs_uri\n",
    "\n",
    "def inspect_gcs_data(uri):\n",
    "    bucket_name = uri.split(\"/\")[2]\n",
    "    prefix = \"/\".join(uri.split(\"/\")[3:])\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=prefix))\n",
    "    \n",
    "    print(f\"Found {len(blobs)} files in {uri}\")\n",
    "    \n",
    "    for blob in blobs:\n",
    "        print(f\"\\nChecking file: {blob.name} ({blob.size} bytes)\")\n",
    "        content = blob.download_as_text()\n",
    "        lines = content.strip().split('\\n')\n",
    "        print(f\"Total lines: {len(lines)}\")\n",
    "        \n",
    "        if len(lines) > 0:\n",
    "            first_line = json.loads(lines[0])\n",
    "            print(\"First record keys:\", first_line.keys())\n",
    "            if 'embedding' in first_line:\n",
    "                print(\"Vector dim:\", len(first_line['embedding']))\n",
    "            else:\n",
    "                print(\"CRITICAL ERROR: 'embedding' key missing!\")\n",
    "\n",
    "inspect_gcs_data(FAILED_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0060b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Index (takes time)...\n",
      "Index params: {'dimensions': 3072, 'approximate_neighbors_count': 150, 'leaf_node_embedding_count': 100, 'leaf_nodes_to_search_percent': 10, 'distance_measure_type': 'DOT_PRODUCT_DISTANCE', 'contents_delta_uri': 'gs://vecor_embedding002/init_vectors_8be84b8c-5da2-4b61-b02e-885a53cbb735/'}\n",
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/908027936872/locations/us-central1/indexes/7164041733611716608/operations/8405778952831893504\n",
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/908027936872/locations/us-central1/indexes/7164041733611716608/operations/8405778952831893504\n",
      "MatchingEngineIndex created. Resource name: projects/908027936872/locations/us-central1/indexes/7164041733611716608\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/908027936872/locations/us-central1/indexes/7164041733611716608')\n",
      "MatchingEngineIndex created. Resource name: projects/908027936872/locations/us-central1/indexes/7164041733611716608\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/908027936872/locations/us-central1/indexes/7164041733611716608')\n",
      "Creating Endpoint\n",
      "Creating Endpoint\n",
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560/operations/4967280637334519808\n",
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560/operations/4967280637334519808\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560')\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560')\n",
      "Deploying Index to Endpoint\n",
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560\n",
      "Deploying Index to Endpoint\n",
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560/operations/5203719617771470848\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560/operations/5203719617771470848\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560\n",
      "deployment Completed\n",
      "Completed deploying index\n",
      "deployment Completed\n",
      "Completed deploying index\n"
     ]
    }
   ],
   "source": [
    "index_endpoint= create_and_deploy_index(gcs_uri)\n",
    "print('Completed deploying index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6260870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint.MatchingEngineIndexEndpoint object at 0x0000017710098830> \n",
       "resource name: projects/908027936872/locations/us-central1/indexEndpoints/6066896254858690560"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95759966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: [[MatchNeighbor(id='195b5f53-90c8-4d3d-aab8-b9945cae468b', distance=0.6637460589408875, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]), MatchNeighbor(id='5d0458ad-c1f5-4bce-98cf-c4e964764b98', distance=0.6550101041793823, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]), MatchNeighbor(id='d78cd1a2-5669-4d2d-9705-d6d24f867768', distance=0.6543611288070679, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]), MatchNeighbor(id='ca3d2ff9-8201-4631-8e9c-3dbf5d0fc592', distance=0.6533534526824951, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]), MatchNeighbor(id='3bf6cab0-998b-417a-9ae9-4102ee0696d8', distance=0.6513634920120239, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[])]]\n",
      "Login successed\n"
     ]
    }
   ],
   "source": [
    "results = search_vertex_ai(\n",
    "    query=\"machine learning\", \n",
    "    index_endpoint=index_endpoint,      \n",
    "    deployed_index_id=\"soumya_deployed_v1\", \n",
    "    bucket_name=BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44d78457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'tables': [], 'images': [], 'score': 0.6637460589408875}, page_content='Advertisement and Publicity Team\\n\\n1.Shradhanjali Das (124AI0024) 2.Gangala Tanishka (124AI0005) 3.Suraj Kumar Sahu (124AI0013)\\n\\nPROBLEM STATEMENT :\\n\\nTitle: Automated Fake News Detection System Development\\n\\nIn the modern digital age, the rapid dissemination of unverified and misleading information—commonly referred to as \"fake news\" or \"information disorder\"—poses a critical threat to democratic processes, public health, and social stability. The sheer volume and velocity of content generated daily across platforms make manual verification impossible, necessitating advanced automated solutions. The ability of malicious actors to use sophisticated techniques to spread hoaxes and disinformation undermines trust in legitimate news sources and contributes to polarization.\\n\\nThis project focuses on the development of a robust and scalable Fake News Detection (FND) system designed to identify and flag deceptive articles. The system leverages a curated, static training corpus, such as the ISOT dataset (containing news articles up to late 2019 across politics, government, and US news), to learn core patterns of disinformation. For real-time prediction and adaptability to current events, the system integrates live data acquisition through API calls to major news sources. The training data, comprising real and fabricated news articles, encompasses textual content and their ground-truth labels, spanning multiple domains (including political, economic, etc.), allowing for the creation of a generalized detection model.\\n\\nThe primary goal of this analysis is to design, implement, and evaluate a machine learning pipeline capable of accurately distinguishing between legitimate and deceptive news content in real time.'),\n",
       "  0.6637460589408875),\n",
       " (Document(metadata={'tables': [], 'images': [], 'score': 0.6550101041793823}, page_content=\"Data Splitting\\n\\nThe processed dataset was partitioned into training and testing sets to evaluate the model's performance on unseen data. A standard 80/20 split was utilized, with a specific random state to ensure reproducibility.\\n\\nModel Selection and Training:\\n\\nThis section now incorporates the final details on the model instantiation, the training process, the use of custom metrics, and the crucial early stopping mechanism.\\n\\nCustom Dataset Implementation:\\n\\nA custom class inheriting from the torch.utils.data.Dataset abstract class was implemented to efficiently handle the large volume of preprocessed text tokens and labels. This class serves two main purposes:\\n\\n1. Data Structure: It wraps the tokenized inputs (input_ids, attention_mask) and the target labels.\\n\\n2. Indexing: It allows easy indexing to retrieve a specific sample, which is necessary for the PyTorch DataLoader to iterate over the data in batches during training.\\n\\nThe custom Dataset prepares the encoded data (train_enc, val_enc) and the label arrays (y_train, y_test) into the format expected by the Hugging Face Trainer API.\"),\n",
       "  0.6550101041793823),\n",
       " (Document(metadata={'tables': [], 'images': [], 'score': 0.6543611288070679}, page_content='DATA COLLECTION AND DATASET USED :\\n\\nIntroduction:\\n\\nThe ISOT Fake News Dataset is a prominent open-source dataset maintained by the University of Victoria\\'s Information Security and Object Technology (ISOT) Research Lab. It is widely used in the field of Natural Language Processing (NLP) and machine learning for the development of automated fake news detection systems. The dataset is designed for binary classification tasks, distinguishing between truthful journalism and fabricated misinformation.\\n\\nisot-fake-news-dataset\\n\\nDataset Link for the dataset used is :\\n\\nThe dataset consists of approximately 45,000 news articles, evenly balanced between \"Fake\" and \"Real\" news.\\n\\n● Total Records: ~44,898 articles (21,417 True / 23,481 Fake)\\n\\n● Total Features: 4 main text/metadata columns\\n\\n● File Format: CSV\\n\\n● Timeframe: Predominantly 2016–2017 (covering major political events)\\n\\n● Key Characteristics: Unprocessed full-text articles; contains significant noise in the \"Fake\" subset (e.g., varying capitalization, punctuation errors).'),\n",
       "  0.6543611288070679),\n",
       " (Document(metadata={'tables': ['<table><thead><tr><th>Parameter</th><th>Value</th><th>Detail</th></tr></thead><tbody><tr><td>Base Model</td><td>microsoft/deberta-v3- base</td><td>Chosen for its state-of-the- art performance in NLP.</td></tr><tr><td>Optimizer</td><td>AdamW (Implicit)</td><td>Standard high-performance optimizer with weight_decay</td></tr><tr><td>Loss Function</td><td>Cross-Entropy Loss (Implicit)</td><td>Standard loss for the binary classification task.</td></tr><tr><td>Early Stopping</td><td>patience</td><td>Training automatically halts if the validation metric does not improve after 5 epochs, preventing overfitting.</td></tr></tbody></table>', '<table><thead><tr><th>Parameter</th><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>Number of Epochs</td><td>4</td><td>The total number of passes over the training data.</td></tr><tr><td>Batch Size (Train/Eval)</td><td>4</td><td>Small batch size used due to the large memory footprint of the DeBERTa model and maximum sequence length(512).</td></tr><tr><td>Evaluation Strategy</td><td>epoch</td><td>Evaluation is performed after every epoch</td></tr><tr><td>Learning Rate</td><td>2 x 104-5</td><td>A common, effective learning rate for fine-tuning large pre-trained models.</td></tr><tr><td>Weight Decay</td><td>0.01</td><td>L2 regularization applied to prevent overfitting.</td></tr></tbody></table>', '<table><thead><tr><th>Epoch</th><th>Training Loss</th><th>Validation Loss</th><th>Accuracy</th></tr></thead><tbody><tr><td>1</td><td>0.000000</td><td>0.000000</td><td>1.000000</td></tr><tr><td>2</td><td>0.025200</td><td>0.000000</td><td>1.000000</td></tr><tr><td>3</td><td>0.000000</td><td>0.013103</td><td>0.999443</td></tr><tr><td>4</td><td>0.000000</td><td>0.005302</td><td>0.999777</td></tr></tbody></table>'], 'images': [], 'score': 0.6533534526824951}, page_content=\"Model Configuration And Training SetUp:\\n\\nThe model chosen for this task is the microsoft/deberta-v3-base,a robust Transformer-based language model. The classification architecture is completed by adding a standard Sequence Classification Head on top of the base model's pooled output.\\n\\nThe fine-tuning was managed using the Hugging Face Trainer API, which simplifies the optimization process:\\n\\nParameter Value Base Model microsoft/deberta-v3- base Optimizer AdamW (Implicit) Loss Function Cross-Entropy Loss (Implicit) Early Stopping patience Detail Chosen for its state-of-the- art performance in NLP. Standard high-performance optimizer with weight_decay Standard loss for the binary classification task. Training automatically halts if the validation metric does not improve after 5 epochs, preventing overfitting.\\n\\nTraining Arguments and Hyperparameter:\\n\\nParameter Value Number of 4 Epochs Batch Size 4 (Train/Eval) Evaluation epoch Strategy Learning 2 x 10^-5 Rate Weight 0.01 Decay Description The total number of passes over the training data. Small batch size used due to the large memory footprint of the DeBERTa model and maximum sequence length(512). Evaluation is performed after every epoch A common, effective learning rate for fine-tuning large pre-trained models. L2 regularization applied to prevent overfitting.\\n\\nEvaluation Metrics And Results:\\n\\nThe primary metric used to monitor and evaluate the model was Accuracy, defined via a custom compute_metrics function. The training was tracked over four epochs, and the following results demonstrate the model's rapid convergence and high performance on the validation set.\\n\\nEpoch Training Validation Loss Accuracy Loss 1 0.000000 0.000000 1.000000 2 0.025200 0.000000 1.000000 3 0.000000 0.013103 0.999443 4 0.000000 0.005302 0.999777\"),\n",
       "  0.6533534526824951),\n",
       " (Document(metadata={'tables': ['<table><tbody><tr><td>Fake</td><td>23,481</td></tr><tr><td>Total</td><td>44,898</td></tr></tbody></table>'], 'images': [], 'score': 0.6513634920120239}, page_content=\"Dataset Statistics and Integrity:\\n\\nTo provide a complete picture of the dataset used for model\\n\\ntraining, the following statistics were calculated after the initial\\n\\nmerging of the 'True' and 'Fake' source files.\\n\\nClass Distribution:\\n\\nThe dataset is highly balanced across the two primary classification\\n\\nLabel Count True 21,417 Fake 23,481\\n\\nclasses, which is beneficial for avoiding bias during model training.\\n\\nTotal\\n\\n44,898\\n\\nThe nearly equal distribution of 23,481 'fake' and 21,417 'true' records confirms the dataset's excellent balance for a binary\\n\\nclassification task, mitigating the risk of class imbalance skewing\\n\\nthe model's predictive capabilities\"),\n",
       "  0.6513634920120239)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bcfdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk,score = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2b52672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6637460589408875"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a02a8e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advertisement and Publicity Team\\n\\n1.Shradhanjali Das (124AI0024) 2.Gangala Tanishka (124AI0005) 3.Suraj Kumar Sahu (124AI0013)\\n\\nPROBLEM STATEMENT :\\n\\nTitle: Automated Fake News Detection System Development\\n\\nIn the modern digital age, the rapid dissemination of unverified and misleading information—commonly referred to as \"fake news\" or \"information disorder\"—poses a critical threat to democratic processes, public health, and social stability. The sheer volume and velocity of content generated daily across platforms make manual verification impossible, necessitating advanced automated solutions. The ability of malicious actors to use sophisticated techniques to spread hoaxes and disinformation undermines trust in legitimate news sources and contributes to polarization.\\n\\nThis project focuses on the development of a robust and scalable Fake News Detection (FND) system designed to identify and flag deceptive articles. The system leverages a curated, static training corpus, such as the ISOT dataset (containing news articles up to late 2019 across politics, government, and US news), to learn core patterns of disinformation. For real-time prediction and adaptability to current events, the system integrates live data acquisition through API calls to major news sources. The training data, comprising real and fabricated news articles, encompasses textual content and their ground-truth labels, spanning multiple domains (including political, economic, etc.), allowing for the creation of a generalized detection model.\\n\\nThe primary goal of this analysis is to design, implement, and evaluate a machine learning pipeline capable of accurately distinguishing between legitimate and deceptive news content in real time.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cfde54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9866c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_final_ans(chunks, query):\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(model='gemini-2.5-pro', temperature=0.1)\n",
    "\n",
    "        prompt_text = f\"\"\"\n",
    "        Based on the following document context, please answer this question: {query}\n",
    "        \n",
    "        CONTENT_TO_ANALYZE:\n",
    "        \"\"\"\n",
    "        all_images_base64 = []\n",
    "        for content in chunks:\n",
    "            chunk,score=content\n",
    "            \n",
    "            prompt_text += f\"\\n--- Document Fragment {i+1} ---\\n\"\n",
    "            \n",
    "            if hasattr(chunk, \"page_content\"):\n",
    "                try:\n",
    "                    # original_data = json.loads(chunk.metadata['original_content'])\n",
    "                    \n",
    "                    raw_text = chunk.page_content\n",
    "                    if raw_text:\n",
    "                        prompt_text += f\"Text:\\n{raw_text}\\n\\n\"\n",
    "\n",
    "                    tables_html = chunk.metadata.get(\"tables\", [])\n",
    "                    if tables_html:\n",
    "                        prompt_text += 'TABLES:\\n'\n",
    "                        for j, table in enumerate(tables_html):\n",
    "                            prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "                    \n",
    "                    \n",
    "                    imgs = chunk.metadata.get(\"images\", [])\n",
    "                    if imgs:\n",
    "                        all_images_base64.extend(imgs)\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    prompt_text += f\"Text:\\n{chunk.page_content}\\n\\n\"\n",
    "            else:\n",
    "                prompt_text += f\"Text:\\n{chunk.page_content}\\n\\n\"\n",
    "\n",
    "        prompt_text += \"\"\" \n",
    "        INSTRUCTIONS:\n",
    "        Provide a clear, comprehensive answer using the text, tables, and images provided above. \n",
    "        If the documents don't contain sufficient information to answer the question, state: \"I don't have enough information to answer the question.\"\n",
    "        \n",
    "        ANSWER:\"\"\"\n",
    "    \n",
    "        message_content = [{'type': 'text', 'text': prompt_text}]\n",
    "\n",
    "        for img_b64 in all_images_base64:\n",
    "            message_content.append({\"type\": \"image_url\",\"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}})\n",
    "\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer gen failed: {e}\")\n",
    "        return 'Sorry, I encountered an error generating the answer.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebb8d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer gen failed: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sorry, I encountered an error generating the answer.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_final_ans(query,results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
