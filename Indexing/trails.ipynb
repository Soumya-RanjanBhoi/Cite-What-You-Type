{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS,Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.output_parsers import StructuredOutputParser,ResponseSchema\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableParallel,RunnableLambda,RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "print(shutil.which(\"tesseract\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"unstructured[all-docs]\" pillow lxml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install unstructured==0.18.21\n",
    "# %pip install unstructured-client==0.42.4\n",
    "# %pip install unstructured.pytesseract==0.3.15\n",
    "# %pip install unstructured_inference==1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"D:/Cite-What-You-Type/pdfs/22-25 Clustering, K-means, DBSCAN.pdf\"\n",
    "output_dir =\"images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d54d92",
   "metadata": {},
   "source": [
    "## Partition-1 : atomic elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_elements(filepath: str,output_dir:str) -> list:\n",
    "    elements = partition_pdf(\n",
    "        filename=filepath,\n",
    "        strategy=\"hi_res\",\n",
    "        infer_table_structure=True,\n",
    "        extract_image_block_types=['Image'],\n",
    "        extract_image_block_to_payload=True,\n",
    "        extract_image_block_output_dir=output_dir\n",
    "    )\n",
    "    print(\"Total elements extracted -> \",len(elements))\n",
    "    return elements\n",
    "\n",
    "elements = extract_pdf_elements(filepath,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1794135",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([str(type(el))for el in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements[180].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [elem for elem in elements if elem.category == \"Table\"]\n",
    "images = [elem for elem in elements if elem.category == \"Image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d675a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f64b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd04f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(output_dir, image_list):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i, img_obj in enumerate(image_list):\n",
    "        bs64_str = img_obj.to_dict()['metadata']['image_base64']\n",
    "        image_data = base64.b64decode(bs64_str)\n",
    "\n",
    "        path = os.path.join(output_dir, f\"image-{i}.png\")\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(image_data)\n",
    "\n",
    "get_image(output_dir,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dab910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_table(tables):\n",
    "    for i,table_obj in enumerate(tables):\n",
    "        display(HTML(tables[i].metadata.text_as_html))\n",
    "\n",
    "display_table(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44bddfa",
   "metadata": {},
   "source": [
    "## Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_by_title(elements):\n",
    "\n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        max_characters=500,\n",
    "        combine_text_under_n_chars=100\n",
    "    )\n",
    "\n",
    "    print(\"Chunks created: \", len(chunks))\n",
    "    return chunks\n",
    "\n",
    "chunks = create_chunks_by_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[16].metadata.orig_elements[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fcfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[16].metadata.orig_elements[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_content_types(chunk):\n",
    "\n",
    "    content_data ={\n",
    "        \"text\": chunk.text,\n",
    "        \"tables\":[],\n",
    "        \"images\":[],\n",
    "        \"types\":['text']\n",
    "    }\n",
    "\n",
    "    if hasattr(chunk ,\"metadata\") and hasattr(chunk.metadata, \"orig_elements\"):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "\n",
    "            element_type = type(element).__name__\n",
    "\n",
    "            if element_type ==\"Table\":\n",
    "                content_data['types'].append('Table')\n",
    "                table_html = getattr(element.metadata , 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "\n",
    "            elif element_type == \"Image\":\n",
    "                if hasattr(element,\"metadata\") and hasattr(element.metadata ,\"image_base64\"):\n",
    "                    content_data['types'].append(\"Image\")\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "    \n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ai_enhanced_summary(text: str, tables: List[str], images: List[str]) -> str:\n",
    "    try:\n",
    "        model = ChatGoogleGenerativeAI(model='gemini-2.5-pro', temperature=0.3)\n",
    "\n",
    "        prompt_text = f\"\"\"\n",
    "        You are an AI assistant creating a searchable description for document retrieval.\n",
    "\n",
    "        --- CONTENT TO ANALYZE ---\n",
    "        \n",
    "        TEXT CONTENT:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        if tables:\n",
    "            prompt_text += \"\\nTABLES:\\n\"\n",
    "            for i, table_obj in enumerate(tables):\n",
    "                prompt_text += f\"Table {i+1}:\\n{table_obj}\\n\\n\"\n",
    "\n",
    "        prompt_text += \"\"\" \n",
    "        --- YOUR TASK ---\n",
    "        Generate a comprehensive, searchable description of the content above. \n",
    "        Focus on creating metadata that will help a search engine find this document.\n",
    "        \n",
    "        Cover these 5 points:\n",
    "        1. Key facts, exact numbers, and data points (from text and tables)\n",
    "        2. Main topics and concepts discussed\n",
    "        3. Questions this content could answer (e.g., \"What is the revenue for Q3?\")\n",
    "        4. Visual Content Analysis (describe charts, diagrams, and patterns in the attached images)\n",
    "        5. Alternative keywords or synonyms users might search for.\n",
    "\n",
    "        Prioritize findability over brevity.\n",
    "        \n",
    "        SEARCHABLE DESCRIPTION: \n",
    "        \"\"\"\n",
    "\n",
    "        message_content = [{'type': 'text', 'text': prompt_text}]\n",
    "\n",
    "        for image_base64 in images:\n",
    "            clean_base64 = image_base64.strip()\n",
    "            \n",
    "            message_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{clean_base64}\"}})\n",
    "\n",
    "        message = HumanMessage(content=message_content)\n",
    "        \n",
    "        response = model.invoke([message])\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"AI Summarization Failed: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7974699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks):\n",
    "\n",
    "    print(\"...Processing Chunk ...\")\n",
    "\n",
    "    langchain_document=[]\n",
    "\n",
    "    total_chunk = len(chunks)\n",
    "\n",
    "    for i , chunk in enumerate(chunks):\n",
    "        current_chunk = i+1\n",
    "        print(f\"Processed Chunk {current_chunk}/{total_chunk}\")\n",
    "\n",
    "        content_data =seperate_content_types(chunk)\n",
    "\n",
    "        print(f'Types Found: ',content_data['types'])\n",
    "        print(f\"Tables: {len(content_data['tables'])} , Image: {len(content_data['images'])}\")\n",
    "\n",
    "        enhanced_cnt =\"\"\n",
    "\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(\"Creating Summary...\")\n",
    "            try:\n",
    "                enhanced_cnt = create_ai_enhanced_summary(content_data['text'],content_data['tables'],content_data['images'])\n",
    "\n",
    "                if enhanced_cnt:\n",
    "                    print(\"Successfully Summarized\")\n",
    "                    print(f\"Preview:{enhanced_cnt[:100]}...\")\n",
    "                else:\n",
    "                    enhanced_cnt = content_data['text']\n",
    "            except Exception as e:\n",
    "                print(f\"AI Summary Failed: {e}\")\n",
    "                enhanced_cnt= content_data['text']\n",
    "\n",
    "        else:\n",
    "            print(\"No tables or Image Found\")\n",
    "            enhanced_cnt= content_data['text']\n",
    "\n",
    "        doc = Document(\n",
    "            page_content=enhanced_cnt,\n",
    "            metadata={\n",
    "                \"original_content\":json.dumps({\n",
    "                    'raw_text':content_data['text'],\n",
    "                    \"table_html\":content_data['tables'],\n",
    "                    \"image_base64\":content_data['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "\n",
    "        langchain_document.append(doc)\n",
    "\n",
    "    print(f\"Processed {len(langchain_document)} chunks\")\n",
    "    return langchain_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12caa13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks = summarize_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_chunks_to_json(chunks,filename =\"Chunks_exported.json\"):\n",
    "\n",
    "    export=[]\n",
    "\n",
    "    for i , doc in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"chunk_id\":i+1,\n",
    "            'enhanced_content': doc.page_content,\n",
    "            'metadata':{\n",
    "                'original_content':json.loads(doc.metadata.get(\"original_content\",\"{}\"))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        export.append(chunk_data)\n",
    "\n",
    "    with open(filename ,'w',encoding='utf-8') as f:\n",
    "        json.dump(export,f,indent=6,ensure_ascii=False)\n",
    "\n",
    "    print(f'Exported {len(export)} chunks to {filename}')\n",
    "    return export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = export_chunks_to_json(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents,persist_directory=\"FAISS/pdfsVector\"):\n",
    "\n",
    "    embedding_model= GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
    "\n",
    "    vectorstore= FAISS.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_metadata={'hnsw:space':\"cosine\"}\n",
    "    )\n",
    "\n",
    "    print('finished')\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf994b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db=create_vector_store(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c421a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_final_ans(chunks, query):\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(model='gemini-2.5-pro', temperature=0.1)\n",
    "\n",
    "        prompt_text = f\"\"\"\n",
    "        Based on the following document context, please answer this question: {query}\n",
    "        \n",
    "        CONTENT_TO_ANALYZE:\n",
    "        \"\"\"\n",
    "        all_images_base64 = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prompt_text += f\"\\n--- Document Fragment {i+1} ---\\n\"\n",
    "            \n",
    "            if hasattr(chunk, \"metadata\") and \"original_content\" in chunk.metadata:\n",
    "                try:\n",
    "                    original_data = json.loads(chunk.metadata['original_content'])\n",
    "                    \n",
    "                    raw_text = original_data.get(\"raw_text\", \"\")\n",
    "                    if raw_text:\n",
    "                        prompt_text += f\"Text:\\n{raw_text}\\n\\n\"\n",
    "\n",
    "                    tables_html = original_data.get(\"tables_html\", [])\n",
    "                    if tables_html:\n",
    "                        prompt_text += 'TABLES:\\n'\n",
    "                        for j, table in enumerate(tables_html):\n",
    "                            prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "                    \n",
    "                    \n",
    "                    imgs = original_data.get(\"images_base64\", [])\n",
    "                    if imgs:\n",
    "                        all_images_base64.extend(imgs)\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    prompt_text += f\"Text:\\n{chunk.page_content}\\n\\n\"\n",
    "            else:\n",
    "                prompt_text += f\"Text:\\n{chunk.page_content}\\n\\n\"\n",
    "\n",
    "        prompt_text += \"\"\" \n",
    "        INSTRUCTIONS:\n",
    "        Provide a clear, comprehensive answer using the text, tables, and images provided above. \n",
    "        If the documents don't contain sufficient information to answer the question, state: \"I don't have enough information to answer the question.\"\n",
    "        \n",
    "        ANSWER:\"\"\"\n",
    "    \n",
    "        message_content = [{'type': 'text', 'text': prompt_text}]\n",
    "\n",
    "        for img_b64 in all_images_base64:\n",
    "            message_content.append({\"type\": \"image_url\",\"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}})\n",
    "\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer gen failed: {e}\")\n",
    "        return 'Sorry, I encountered an error generating the answer.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b933bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"what are the steps for DBSCAN Clustering\"\n",
    "\n",
    "retriver = db.as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "chunk_query=retriver.invoke(query)\n",
    "\n",
    "final_ans = gen_final_ans(chunk_query,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    \"query\":RunnablePassthrough(),\n",
    "    \"context\": retriver \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain.invoke(\"what are the steps for DBSCAN Clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b336aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = parallel_chain | RunnableLambda(lambda inputs: gen_final_ans(inputs['context'], inputs['query']))\n",
    "\n",
    "query = \"what are the steps for DBSCAN Clustering\"\n",
    "final_answer = chain.invoke(query)\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f8ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9eb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
