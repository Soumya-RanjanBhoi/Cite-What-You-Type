{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user google-cloud-aiplatform>=1.29.0 google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c22f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5cea5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Cite-What-You-Type'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5b54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749ae82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from google.cloud import storage\n",
    "import uuid\n",
    "from langchain_google_vertexai import VectorSearchVectorStore\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain_core.documents import Document\n",
    "from Multi_Modal.chunking import get_chunks\n",
    "from Multi_Modal.SeperationAndSummarization import summarize_chunks\n",
    "from conversion import convert_to_pdf\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b1727f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615f3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "REGION = os.environ['REGION']\n",
    "BUCKET_NAME=os.environ['BUCKET_NAME']\n",
    "INDEX_DISPLAY_NAME=os.environ['INDEX_DISPLAY_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f27d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID,location=REGION)\n",
    "\n",
    "embeddings= GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_vector_to_gcs(documents, bucket_name=BUCKET_NAME):\n",
    "\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    print(\"Completed Login\")\n",
    "\n",
    "    texts = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        try:\n",
    "            raw_text = \"\"\n",
    "            if hasattr(doc, \"metadata\") and \"original_content\" in doc.metadata:\n",
    "                content_str = doc.metadata.get(\"original_content\")\n",
    "                if content_str:\n",
    "                    data = json.loads(content_str)\n",
    "                    raw_text = data.get('raw_text', \"\")\n",
    "            \n",
    "            if not raw_text:\n",
    "                raw_text = doc.page_content \n",
    "\n",
    "            texts.append(raw_text)\n",
    "            valid_indices.append(i)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing doc {i}: {e}\")\n",
    "            texts.append(\"\") \n",
    "\n",
    "    print(f\"Generating embeddings for {len(texts)} documents...\")\n",
    "    vectors = embeddings.embed_documents(texts)\n",
    "    if vectors:\n",
    "        print(f\"!!! ACTUAL VECTOR DIMENSION: {len(vectors[0])} !!!\") \n",
    "\n",
    "\n",
    "    vertex_content = []\n",
    "    \n",
    "    for i, doc_index in enumerate(valid_indices):\n",
    "        doc = documents[doc_index]\n",
    "        \n",
    "        docs_id = str(uuid.uuid4()) \n",
    "        \n",
    "        metadata_blob = bucket.blob(f\"docstore/{docs_id}.json\")\n",
    "        \n",
    "        try:\n",
    "            content = json.loads(doc.metadata.get(\"original_content\", \"{}\"))\n",
    "        except:\n",
    "            content = {}\n",
    "\n",
    "        bucket_content = {\n",
    "            'id': docs_id,\n",
    "            'raw_text': content.get('raw_text', texts[i]), \n",
    "            'table_as_html': content.get('table_html', []),\n",
    "            'image_base64': content.get('image_base64', [])\n",
    "        }\n",
    "        \n",
    "        metadata_blob.upload_from_string(json.dumps(bucket_content))\n",
    "\n",
    "        vertex_record = {\n",
    "            \"id\": docs_id,\n",
    "            \"embedding\": vectors[i] \n",
    "        }\n",
    "        vertex_content.append(json.dumps(vertex_record))\n",
    "\n",
    "    vector_data = \"\\n\".join(vertex_content)\n",
    "    \n",
    "    unique_folder = f\"init_vectors_{uuid.uuid4()}\"\n",
    "    \n",
    "    blob_name = f\"{unique_folder}/vectors.json\"\n",
    "    vector_blob = bucket.blob(blob_name)\n",
    "    vector_blob.upload_from_string(vector_data)\n",
    "\n",
    "    print(f\"Success! Metadata in gs://{bucket_name}/docstore/\")\n",
    "    print(f\"Vectors ready in gs://{bucket_name}/{unique_folder}/\")\n",
    "    \n",
    "    gcs_uri = f\"gs://{bucket_name}/{unique_folder}/\"\n",
    "    return gcs_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(docs[0],\"page_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].metadata[\"original_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31839738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files(documents: List[Document], index_name: str, bucket_name: str = BUCKET_NAME):\n",
    "\n",
    "\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    pc = Pinecone()\n",
    "\n",
    "    print(\"Login successful to GCP & Pinecone\")\n",
    "\n",
    "    texts = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        try:\n",
    "            if hasattr(doc, \"page_content\") and doc.page_content:\n",
    "                texts.append(doc.page_content)\n",
    "            else:\n",
    "                texts.append(\"\")  \n",
    "        except Exception as e:\n",
    "            print(f\"Error while parsing document {i}, error:\", e)\n",
    "            raise  \n",
    "\n",
    "    print(f\"Generating embeddings for {len(texts)} documents...\")\n",
    "    vectors = embeddings.embed_documents(texts)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfdif_matrix=vectorizer.fit_transform(texts)\n",
    "    \n",
    "    if not vectors:\n",
    "        raise ValueError(\"No embeddings generated; check your documents/embeddings model.\")\n",
    "\n",
    "    print(f\"!!! ACTUAL VECTOR DIMENSION: {len(vectors[0])} !!!\")\n",
    "    dimension = len(vectors[0])\n",
    "\n",
    "    print(\"Initializing index in Pinecone\")\n",
    "    existing_indexes = [i[\"name\"] for i in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"gcp\", region=\"us-central1\")\n",
    "        )\n",
    "        print(f\"Created Pinecone index: {index_name}\")\n",
    "    else:\n",
    "        print(f\"Pinecone index '{index_name}' already exists\")\n",
    "\n",
    "\n",
    "    pinecone_docs: List[Document] = []\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_id = uuid.uuid4().hex[:15]\n",
    "\n",
    "        try:\n",
    "            original_content = doc.metadata.get(\"original_content\", \"{}\")\n",
    "            content = json.loads(original_content) if isinstance(original_content, str) else original_content\n",
    "        except Exception:\n",
    "            content = {}\n",
    "\n",
    "        bucket_content = {\n",
    "            \"id\": doc_id,\n",
    "            \"raw_text\": content.get(\"raw_text\", texts[i]),\n",
    "            \"table_as_html\": content.get(\"table_html\", []),\n",
    "            \"image_base64\": content.get(\"image_base64\", []),\n",
    "        }\n",
    "\n",
    "        metadata_blob = bucket.blob(f\"docstore/{doc_id}.json\")\n",
    "        metadata_blob.upload_from_string(json.dumps(bucket_content))\n",
    "\n",
    "        pinecone_doc = Document(\n",
    "            page_content=texts[i],\n",
    "            metadata={\n",
    "                \"id\": doc_id,\n",
    "                \"gcs_uri\": f\"gs://{bucket_name}/docstore/{doc_id}.json\",\n",
    "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            },\n",
    "        )\n",
    "        pinecone_docs.append(pinecone_doc)\n",
    "\n",
    "    print(\"Upserting documents into Pinecone via LangChain...\")\n",
    "    vector_store = PineconeVectorStore.from_documents(\n",
    "        documents=pinecone_docs,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name,\n",
    "    )\n",
    "\n",
    "    print(\"Ingestion complete: GCS + Pinecone hybrid store is ready.\")\n",
    "    return tfdif_matrix,vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f13098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_deploy_index(gcs_uri):\n",
    "\n",
    "    import traceback\n",
    "    print(\"Creating Index (takes time)...\")\n",
    "\n",
    "    dimensions = 3072\n",
    "    approximate_neighbors_count = 150\n",
    "    leaf_node_embedding_count = 100\n",
    "    leaf_nodes_to_search_percent = 10\n",
    "    distance_measure_type = \"DOT_PRODUCT_DISTANCE\"\n",
    "\n",
    "    print(\"Index params:\", {\n",
    "        'dimensions': dimensions,\n",
    "        'approximate_neighbors_count': approximate_neighbors_count,\n",
    "        'leaf_node_embedding_count': leaf_node_embedding_count,\n",
    "        'leaf_nodes_to_search_percent': leaf_nodes_to_search_percent,\n",
    "        'distance_measure_type': distance_measure_type,\n",
    "        'contents_delta_uri': gcs_uri,\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "            display_name=INDEX_DISPLAY_NAME,\n",
    "            contents_delta_uri=gcs_uri,\n",
    "            dimensions=dimensions,\n",
    "            approximate_neighbors_count=approximate_neighbors_count,\n",
    "            leaf_node_embedding_count=leaf_node_embedding_count,\n",
    "            leaf_nodes_to_search_percent=leaf_nodes_to_search_percent,\n",
    "            distance_measure_type=distance_measure_type,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Index creation failed. Exception repr:\", repr(e))\n",
    "        try:\n",
    "            print(\"Exception type:\", type(e))\n",
    "            if hasattr(e, 'errors'):\n",
    "                print(\"e.errors:\", e.errors)\n",
    "        except Exception as diag_exc:\n",
    "            print(\"Error printing exception attributes:\", diag_exc)\n",
    "        print(\"Full traceback:\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"Creating Endpoint\")\n",
    "    try:\n",
    "        my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "            display_name=f\"{INDEX_DISPLAY_NAME}_endpoint\",\n",
    "            public_endpoint_enabled=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Endpoint creation failed:\", repr(e))\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"Deploying Index to Endpoint\")\n",
    "    try:\n",
    "        my_index_endpoint.deploy_index(\n",
    "            index=my_index,\n",
    "            deployed_index_id=\"soumya_deployed_v1\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Deploy failed:\", repr(e))\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    print(\"deployment Completed\")\n",
    "    return my_index_endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vertex_ai(query,index_endpoint, deployed_index_id, bucket_name=BUCKET_NAME):\n",
    "\n",
    "    query_emb = embeddings.embed_query(query)\n",
    "\n",
    "    response = index_endpoint.find_neighbors(\n",
    "        deployed_index_id=deployed_index_id,\n",
    "        queries = [query_emb],\n",
    "        num_neighbors=5\n",
    "    )\n",
    "\n",
    "    print(\"Response:\",response)\n",
    "\n",
    "\n",
    "    results=[]\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    print(\"Login successed\")\n",
    "\n",
    "    for neighbor in response[0]:\n",
    "        doc_id = neighbor.id\n",
    "        score = neighbor.distance\n",
    "\n",
    "        blob = bucket.blob(f\"docstore/{doc_id}.json\")\n",
    "        if blob.exists():\n",
    "            data = json.loads(blob.download_as_text())\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=data.get('raw_text', \"\"), \n",
    "                metadata={\n",
    "                    \"tables\": data.get('table_as_html', []),\n",
    "                    \"images\": data.get('image_base64', []), \n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "        results.append(doc)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8d58b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pdfs/22-25 Clustering, K-means, DBSCAN.pdf\"\n",
    "output_dir = \"temp_uploads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e5e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7355fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir= Path(output_dir)\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7737a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1= \"Explain the workflow of the project. explain the tech stack used to build the project and tell how the query is procceed when a query is given to the final model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a727c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Chunks\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elements extracted ->  352\n",
      "Chunks created:  25\n",
      "Chunks Created\n",
      "summarize_chunks\n",
      "...Processing Chunk ...\n",
      "Processed Chunk 1/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**1. Key facts, exact numbers, and data points**\n",
      "*   **Document Title:** Clustering: Introduction, K...\n",
      "Processed Chunk 2/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**1. Key facts, exact numbers, and data points:**\n",
      "*   **Document Title:** \"Clustering techniques\"\n",
      "* ...\n",
      "Processed Chunk 3/25\n",
      "Types Found:  ['Table', 'text', 'Image']\n",
      "Tables: 1 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:### SEARCHABLE DESCRIPTION\n",
      "\n",
      "**1. Key Facts, Exact Numbers, and Data Points**\n",
      "*   **Definition:** Clu...\n",
      "Processed Chunk 4/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "*   **Document:** Pre...\n",
      "Processed Chunk 5/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 2\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "*   **Author/Presente...\n",
      "Processed Chunk 6/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 4\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "*   **Algorithm:** K-...\n",
      "Processed Chunk 7/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 3\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "*   **Algorithm:** K-...\n",
      "Processed Chunk 8/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 2\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key Facts, Exact Numbers, and Data Points:**\n",
      "*   **Document Source...\n",
      "Processed Chunk 9/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 3\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:### **SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key Facts, Exact Numbers, and Data Points:**\n",
      "*   **Document So...\n",
      "Processed Chunk 10/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 5\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**1. Key facts, exact numbers, and data points**\n",
      "*   **Document Source:** Presentation by Puneet Kum...\n",
      "Processed Chunk 11/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 4\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**1. Key facts, exact numbers, and data points (from text and tables)**\n",
      "*   **Source:** Presentation...\n",
      "Processed Chunk 12/25\n",
      "Types Found:  ['Table', 'text', 'Image']\n",
      "Tables: 1 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:### Searchable Description\n",
      "\n",
      "**1. Key Facts, Exact Numbers, and Data Points**\n",
      "\n",
      "This document details ...\n",
      "Processed Chunk 13/25\n",
      "Types Found:  ['Table', 'text', 'Image']\n",
      "Tables: 1 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:### 1. Key facts, exact numbers, and data points (from text and tables)\n",
      "*   **Document Title/Context...\n",
      "Processed Chunk 14/25\n",
      "Types Found:  ['text']\n",
      "Tables: 0 , Image: 0\n",
      "No tables or Image Found\n",
      "Processed Chunk 15/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "*   **Algorithm:** DB...\n",
      "Processed Chunk 16/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**1. Key facts, exact numbers, and data points:**\n",
      "*   **Document Source:** Presentation slide 29 out...\n",
      "Processed Chunk 17/25\n",
      "Types Found:  ['text']\n",
      "Tables: 0 , Image: 0\n",
      "No tables or Image Found\n",
      "Processed Chunk 18/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 2\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "*   Source: Presentat...\n",
      "Processed Chunk 19/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key Facts, Exact Numbers, and Data Points:**\n",
      "*   **Algorithm:** DB...\n",
      "Processed Chunk 20/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 4\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "*   **Document Source...\n",
      "Processed Chunk 21/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "*   **Algorithm:** DB...\n",
      "Processed Chunk 22/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:**SEARCHABLE DESCRIPTION:**\n",
      "\n",
      "**1. Key facts, exact numbers, and data points:**\n",
      "This document provide...\n",
      "Processed Chunk 23/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:### **SEARCHABLE DESCRIPTION**\n",
      "\n",
      "**1. Key Facts, Exact Numbers, and Data Points:**\n",
      "\n",
      "This document pro...\n",
      "Processed Chunk 24/25\n",
      "Types Found:  ['text', 'Image']\n",
      "Tables: 0 , Image: 1\n",
      "Creating Summary...\n",
      "Successfully Summarized\n",
      "Preview:### **SEARCHABLE DESCRIPTION**\n",
      "\n",
      "**1. Key Facts, Exact Numbers, and Data Points**\n",
      "\n",
      "*   **Document Tit...\n",
      "Processed Chunk 25/25\n",
      "Types Found:  ['text']\n",
      "Tables: 0 , Image: 0\n",
      "No tables or Image Found\n",
      "Processed 25 chunks\n",
      "Summarization Completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Chunks\")\n",
    "chunks = get_chunks(file_path)\n",
    "print(\"Chunks Created\")\n",
    "\n",
    "print(\"summarize_chunks\")\n",
    "docs = summarize_chunks(chunks)\n",
    "print(\"Summarization Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "283f6645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**1. Key facts, exact numbers, and data points**\\n*   **Document Title:** Clustering: Introduction, K-Mean, and DBSCAN\\n*   **Author:** Puneet Kumar Jain, PhD\\n*   **Author\\'s Affiliation:** Assistant Professor, Computer Science and Engineering (CSE) Department, National Institute of Technology Rourkela (NIT Rourkela).\\n*   **Referenced Resources & Authors:**\\n    *   **Book:** \"Introduction to Data Mining\" by Pang-Ning Tan (Michigan State University), Michael Steinbach (University of Minnesota), and Vipin Kumar (University of Minnesota). Resource URL: http://www-users.cs.umn.edu/~kumar/dmbook/index.php\\n    *   **Presentation 1:** http://www.cse.ust.hk/~qyang/337/slides/cluster.ppt (Hong Kong University of Science and Technology)\\n    *   **Presentation 2:** http://www2.cs.uh.edu/~ceick/ML/Topic9.ppt (University of Houston)\\n    *   **Authors/Researchers:** www.cs.uiuc.edu/~hanj (Jiawei Han, University of Illinois Urbana-Champaign) and Martin Pfeifle (www.dbs.informatik.uni-muenchen.de, Ludwig Maximilian University of Munich).\\n\\n**2. Main topics and concepts discussed**\\n*   **Primary Subject:** Clustering, a fundamental technique in data mining and unsupervised machine learning.\\n*   **Specific Algorithms:** The document provides an introduction to clustering and focuses on two specific algorithms: K-Means (a partitioning, centroid-based method) and DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\\n*   **Broader Fields:** Data Mining, Machine Learning (ML), Computer Science, Data Analysis, Pattern Recognition.\\n*   **Content Type:** Likely academic lecture slides, course material, or a presentation from the CSE Department at NIT Rourkela.\\n\\n**3. Questions this content could answer**\\n*   What is clustering in the context of data mining?\\n*   How does the K-Means clustering algorithm work?\\n*   What is the DBSCAN algorithm and what are its principles?\\n*   What is the difference between K-Means and DBSCAN?\\n*   Who is Dr. Puneet Kumar Jain of NIT Rourkela?\\n*   What are some recommended academic resources or textbooks for learning about clustering and data mining?\\n*   Where can I find lecture materials on clustering from universities like NIT Rourkela, HKUST, or the University of Houston?\\n\\n**4. Visual Content Analysis**\\n*   **Image Description:** The image displays the official logo of the National Institute of Technology Rourkela (NIT Rourkela).\\n*   **Key Elements:** The logo is circular and features a prominent rust-colored gear or cogwheel, symbolizing engineering and industry. Inside the gear, there are several icons: a drawing compass, a set square (triangle), and a red zigzag line resembling a lightning bolt or an electrical symbol. A flaming torch, representing knowledge and enlightenment, is also present.\\n*   **Text:** The name of the institution is written on a banner that curves around the gear, reading \"NATIONAL INSTITUTE OF TECHNOLOGY,\" and in a rectangular box at the bottom, reading \"ROURKELA.\"\\n*   **Overall Theme:** The logo\\'s imagery strongly reflects a focus on engineering, technology, science, and education.\\n\\n**5. Alternative keywords or synonyms users might search for**\\n*   **For Topics:** Cluster analysis, data segmentation, unsupervised learning, partitioning methods, density-based methods, data grouping, pattern analysis, knowledge discovery.\\n*   **For Algorithms:** K-mean, Kmeans, K-means clustering, Lloyd\\'s algorithm, DBSCAN algorithm, spatial clustering.\\n*   **For Institution:** NIT Rourkela, NITR, National Institute of Technology Rourkela India.\\n*   **For Author:** Dr. Puneet Jain, P. K. Jain.\\n*   **For Document Type:** Lecture notes, PowerPoint presentation, course slides, academic paper, educational material, CSE lecture.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f182764",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1].metadata.get('original_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b839a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful to GCP & Pinecone\n",
      "Generating embeddings for 25 documents...\n",
      "!!! ACTUAL VECTOR DIMENSION: 3072 !!!\n",
      "Initializing index in Pinecone\n",
      "Pinecone index 'hybrid-search' already exists\n",
      "Upserting documents into Pinecone via LangChain...\n",
      "Ingestion complete: GCS + Pinecone hybrid store is ready.\n"
     ]
    }
   ],
   "source": [
    "vectorizer,vector_store=upload_files(docs,\"hybrid-search\",BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc=Pinecone()\n",
    "index = pc.Index(host=\"https://soumya-index-fughegr.svc.gcp-us-central1-4a9f.pinecone.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc922bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_pinecone_retriever(query,vectorizer,index_name,top_k=5,bucket_name=BUCKET_NAME):\n",
    "\n",
    "    pc=Pinecone()\n",
    "    index = pc.Index(index_name)\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket=client.bucket(bucket_name)\n",
    "\n",
    "    print(\"Login successful to GCP & Pinecone\")\n",
    "\n",
    "    emb_query = embeddings.embed_query(query)\n",
    "    query_vec=vectorizer.transform(query)\n",
    "    sim=cosine_similarity(query_vec,vectorizer)[0]\n",
    "\n",
    "    res= index.query(\n",
    "        vector=emb_query,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    docs=[]\n",
    "    try:\n",
    "        for result in res['matches']:\n",
    "            try:\n",
    "                if \"metadata\" in result:\n",
    "                    metadata = result.get(\"metadata\",\"{}\")\n",
    "                    if metadata:\n",
    "                        id=metadata.get(\"id\",\"\")\n",
    "                        score= result[\"score\"]\n",
    "                        gcs_uri=metadata.get('gcs_uri',\"\")\n",
    "\n",
    "                        if not gcs_uri:\n",
    "                            continue\n",
    "                    \n",
    "                        path=gcs_uri.replace(\"gs://\",\"\")\n",
    "                        bucket_name_gcs,*blob_parts= path.split(\"/\")\n",
    "                        blob_name =\"/\".join(blob_parts)\n",
    "\n",
    "                        print(path)\n",
    "                        print(bucket_name,\" \",blob_parts)\n",
    "\n",
    "                        bucket = client.bucket(bucket_name_gcs)\n",
    "                        blob = bucket.blob(blob_name)\n",
    "\n",
    "                        if not blob.exists():\n",
    "                            continue\n",
    "\n",
    "                        data = json.loads(blob.download_as_text())\n",
    "\n",
    "                        raw_text = data.get(\"raw_text\", \"\")\n",
    "                        tables   = data.get(\"table_as_html\", [])\n",
    "                        images   = data.get(\"image_base64\", [])\n",
    "\n",
    "                        doc = Document(\n",
    "                            page_content=raw_text,\n",
    "                            metadata={\n",
    "                                \"id\": id,\n",
    "                                \"score\": score,\n",
    "                                \"tables\": tables,\n",
    "                                \"images\": images,\n",
    "                                \"gcs_uri\": gcs_uri,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        docs.append(doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error , Unable to fetch metadata\",e)\n",
    "\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error:{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=hybrid_pinecone_retriever(query1,\"soumya-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87020be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a103243",
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee376c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"upload to GCS\")\n",
    "gcs_uri= upload_vector_to_gcs(docs)\n",
    "print(\"Uploaded Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_endpoint= create_and_deploy_index(gcs_uri)\n",
    "print('Completed deploying index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = aiplatform.MatchingEngineIndexEndpoint.list()\n",
    "for ep in endpoints:\n",
    "    print(f\"Name: {ep.display_name}\")\n",
    "    print(f\"ID: {ep.name}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "    index_endpoint_name=f\"projects/{os.environ['PROJECT_NO']}/locations/{REGION}/indexEndpoints/{os.environ['ENDPOINT_ID']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_vertex_ai(\n",
    "    query=query, \n",
    "    index_endpoint=my_endpoint,      \n",
    "    deployed_index_id=\"soumya_deployed_v1\", \n",
    "    bucket_name=BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfde54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819feea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(results[0],\"page_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ef3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508273ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_final_ans(results,query):\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(model='gemini-2.5-pro', temperature=0.1)\n",
    "\n",
    "        prompt_text = f\"\"\"\n",
    "        Based on the following document context, please answer this question: {query}\n",
    "        \n",
    "        CONTENT_TO_ANALYZE:\n",
    "        \"\"\"\n",
    "        all_images_base64 = []\n",
    "        for i, chunk in enumerate(results):\n",
    "            prompt_text += f\"\\n--- Document Fragment {i+1} ---\\n\"\n",
    "            if hasattr(chunk,\"page_content\"):\n",
    "                raw_text = chunk.page_content\n",
    "                if raw_text:\n",
    "                    prompt_text += f\"Text:\\n{raw_text}\\n\\n\"\n",
    "            \n",
    "            if hasattr(chunk,\"metdata\"):\n",
    "                table= chunk.metadata.get(\"tables\",[])\n",
    "                if table:\n",
    "                    for j , cnt in enumerate(table):\n",
    "                        prompt_text += f\"Table {j+1}:\\n{cnt}\\n\\n\"\n",
    "                imgs = chunk.metadata.get(\"images\",[])\n",
    "                if imgs:\n",
    "                    all_images_base64.extend(imgs)\n",
    "            \n",
    "        prompt_text += \"\"\" \n",
    "            INSTRUCTIONS:\n",
    "            Provide a clear, comprehensive answer using the text, tables, and images provided above. \n",
    "            If the documents don't contain sufficient information to answer the question, state: \"I don't have enough information to answer the question.\"\n",
    "            \n",
    "            ANSWER:\"\"\"\n",
    "        message_content = [{'type': 'text', 'text': prompt_text}]\n",
    "\n",
    "        for img_b64 in all_images_base64:\n",
    "            message_content.append({\"type\": \"image_url\",\"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}})\n",
    "\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer gen failed: {e}\")\n",
    "        return 'Sorry, I encountered an error generating the answer.'\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=gen_final_ans(document,query1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
