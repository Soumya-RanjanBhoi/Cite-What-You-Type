{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab253b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import uuid\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "import torch\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from Multi_Modal.chunking import get_chunks\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import boto3\n",
    "import re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ee74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e924bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"pdfs/Documentation-Project.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c729c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=get_chunks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1717e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_content_types(chunk):\n",
    "\n",
    "    content_data ={\n",
    "        \"text\": chunk.text,\n",
    "        \"tables\":[],\n",
    "        \"images\":[],\n",
    "        \"types\":['text']\n",
    "    }\n",
    "\n",
    "    if hasattr(chunk ,\"metadata\") and hasattr(chunk.metadata, \"orig_elements\"):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "\n",
    "            element_type = type(element).__name__\n",
    "\n",
    "            if element_type ==\"Table\":\n",
    "                content_data['types'].append('Table')\n",
    "                table_html = getattr(element.metadata , 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "\n",
    "            elif element_type == \"Image\":\n",
    "                if hasattr(element,\"metadata\") and hasattr(element.metadata ,\"image_base64\"):\n",
    "                    content_data['types'].append(\"Image\")\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "    \n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11748ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApnaChatModel:\n",
    "    def __init__(self, region=\"us-east-1\"):\n",
    "        self.client = boto3.client(\n",
    "            \"bedrock-runtime\",\n",
    "            region_name=region\n",
    "        )\n",
    "        self.model_id = \"amazon.nova-pro-v1:0\"\n",
    "\n",
    "    def invoke(self, messages, max_tokens=500):\n",
    "        response = self.client.invoke_model(\n",
    "            modelId=self.model_id,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=json.dumps({\n",
    "                \"messages\": messages,\n",
    "                \"inferenceConfig\": {\n",
    "                    \"max_new_tokens\": max_tokens\n",
    "                }\n",
    "            })\n",
    "        )\n",
    "\n",
    "        result = json.loads(response[\"body\"].read())\n",
    "        return result[\"output\"][\"message\"][\"content\"][0][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be776515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ai_enhanced_summary(\n",
    "    text: str,\n",
    "    tables: list,\n",
    "    images: list,\n",
    "    model: ApnaChatModel\n",
    ") -> str:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant creating a searchable description for document retrieval.\n",
    "\n",
    "--- TEXT CONTENT ---\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "    if tables:\n",
    "        prompt += \"\\n--- TABLES ---\\n\"\n",
    "        for i, table in enumerate(tables):\n",
    "            prompt += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "\n",
    "    prompt += \"\"\"\n",
    "--- YOUR TASK ---\n",
    "Generate a comprehensive, searchable description.\n",
    "\n",
    "Cover:\n",
    "1. Key facts, exact numbers, and metrics\n",
    "2. Main topics and concepts\n",
    "3. Questions this content can answer\n",
    "4. Visual insights from images (charts, diagrams, patterns)\n",
    "5. Alternative keywords and synonyms\n",
    "\n",
    "Prioritize searchability over brevity.\n",
    "\"\"\"\n",
    "\n",
    "    content_blocks = []\n",
    "\n",
    "    for img_b64 in images:\n",
    "        content_blocks.append({\n",
    "            \"image\": {\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\": {\n",
    "                    \"bytes\": img_b64.strip()\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    content_blocks.append({\n",
    "        \"text\": prompt\n",
    "    })\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_blocks\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks):\n",
    "\n",
    "    print(\"...Processing Chunks...\")\n",
    "    model = ApnaChatModel()\n",
    "    langchain_document = []\n",
    "\n",
    "    total_chunk = len(chunks)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk = i+1\n",
    "        print(f\"Processed Chunk {current_chunk}/{total_chunk}\")\n",
    "\n",
    "        content_data = seperate_content_types(chunk)\n",
    "\n",
    "        print(f\"Types Found: {content_data['types']}\")\n",
    "        print(f\"Tables: {len(content_data['tables'])}, Images: {len(content_data['images'])}\")\n",
    "\n",
    "        enhanced_cnt = \"\"\n",
    "\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(\"Creating AI Summary...\")\n",
    "            try:\n",
    "                enhanced_cnt = create_ai_enhanced_summary(\n",
    "                    text=content_data['text'],\n",
    "                    tables=content_data['tables'],\n",
    "                    images=content_data['images'],\n",
    "                    model=model\n",
    "                )\n",
    "                print(\"Summary created\")\n",
    "            except Exception as e:\n",
    "                print(f\"Summary failed: {e}\")\n",
    "                enhanced_cnt = content_data['text']\n",
    "        else:\n",
    "            print(\"No tables or Image Found\")\n",
    "            enhanced_cnt = content_data['text']\n",
    "\n",
    "        doc = Document(\n",
    "            page_content=enhanced_cnt,\n",
    "            metadata={\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": content_data['text'],\n",
    "                    \"table_html\": content_data['tables'],\n",
    "                    \"image_base64\": content_data['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "\n",
    "        langchain_document.append(doc)\n",
    "\n",
    "    print(f\"Processed {len(langchain_document)} chunks\")\n",
    "    return langchain_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c505795",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_chunks=summarize_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f249901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanEmbeddings(object):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    def __init__(self, model_id=\"amazon.titan-embed-text-v2:0\"):\n",
    "        self.bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "        self.model_id = model_id\n",
    "        self.dimensions=1024\n",
    "        self.normalize=True\n",
    "    def invoke(self, text):\n",
    "        body = json.dumps({\n",
    "            \"inputText\": text,\n",
    "            \"dimensions\": self.dimensions,\n",
    "            \"normalize\": self.normalize\n",
    "        })\n",
    "        response = self.bedrock.invoke_model(\n",
    "            body=body, modelId=self.model_id, accept=self.accept, contentType=self.content_type\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e37ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = 1024\n",
    "normalize = True\n",
    "    \n",
    "titan_embeddings_v2 = TitanEmbeddings()\n",
    "\n",
    "input_text = sum_chunks[10].page_content\n",
    "embedding = titan_embeddings_v2.invoke(input_text)\n",
    "    \n",
    "print(f\"{input_text=}\")\n",
    "print(f\"{embedding[:10]=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01340ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "emb=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in sum_chunks:\n",
    "    t=re.sub(r\"#{2,3}\", \"\", sum_chunks[0].page_content)\n",
    "    text.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0][1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in text:\n",
    "    emb.append(titan_embeddings_v2.invoke(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd07fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25=BM25Encoder.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61439393",
   "metadata": {},
   "outputs": [],
   "source": [
    "id=uuid.uuid4().hex[1:15]\n",
    "user_id=\"user-\" + str(id)\n",
    "user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore, PineconeSparseVectorStore\n",
    "from pinecone import ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f358cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc=Pinecone()\n",
    "existing_names=[name['name'] for name in pc.list_indexes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53365ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_id not in existing_names:\n",
    "    pc.create_index(\n",
    "        name=user_id,\n",
    "        dimension=1024,\n",
    "        metric=\"dotproduct\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2334ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    \"name\":\"Soumya\",\n",
    "    \"text\":\"tthanks\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeaf03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.put_object(\n",
    "    Bucket=\"soumya-rag\",\n",
    "    Key=f\"{user_id}/{uuid.uuid4().hex[1:15]}.json\",\n",
    "    Body=json.dumps(data),\n",
    "    ContentType=\"application/json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME='soumya-rag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3714c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=pc.Index(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b38f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_to_upsert = []\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367821cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,txt in enumerate(text):\n",
    "    doc_id=uuid.uuid4().hex[1:12]\n",
    "    vector={\n",
    "        \"id\":id,\n",
    "        \"values\":emb[i],\n",
    "        \"sparse_values\":bm25.encode_queries(txt),\n",
    "        \"metadata\":{\n",
    "            \"text\":txt,\n",
    "            \"s3_uri\":f\"s2//{BUCKET_NAME}/{user_id}/{doc_id}/.json\"\n",
    "        }\n",
    "    }\n",
    "    vectors_to_upsert.append(vector)\n",
    "    if len(vectors_to_upsert) > batch_size:\n",
    "        index.upsert(vectors_to_upsert)\n",
    "        vectors_to_upsert = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746221f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Tuple\n",
    "from langchain_classic.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd459b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class TitanLangChainEmbeddings(Embeddings):\n",
    "    def __init__(self, titan_model):\n",
    "        self.titan_model = titan_model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.titan_model.invoke(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.titan_model.invoke(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5098578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHybridRetriever:\n",
    "    def __init__(self, user_id, emb_model, bm25, k=5, alpha=0.7):\n",
    "        self.user_id = user_id\n",
    "        self.emb_model = emb_model\n",
    "        self.bm25 = bm25\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        pc1=Pinecone()\n",
    "        self.index_obj=pc1.Index(self.user_id)\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        dense = self.emb_model.invoke(query)\n",
    "        sparse = self.bm25.encode_queries(query)\n",
    "\n",
    "        dense = [v * self.alpha for v in dense]\n",
    "        sparse[\"values\"] = [v * (1 - self.alpha) for v in sparse[\"values\"]]\n",
    "\n",
    "        res = self.index_obj.query(\n",
    "            vector=dense,\n",
    "            sparse_vector=sparse,\n",
    "            top_k=self.k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "\n",
    "        seen_ids = set()\n",
    "        docs = []\n",
    "\n",
    "        for m in res[\"matches\"]:\n",
    "            doc_id = m[\"id\"]  \n",
    "\n",
    "            if doc_id in seen_ids:\n",
    "                continue\n",
    "\n",
    "            seen_ids.add(doc_id)\n",
    "\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=m[\"metadata\"][\"text\"],\n",
    "                    metadata={\n",
    "                        **m[\"metadata\"],\n",
    "                        \"id\": doc_id\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2613ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(document:List[Document],user_id,bucket_name):\n",
    "\n",
    "    try:\n",
    "        obj= boto3.client(\"s3\")\n",
    "        print(\"logined s3\")\n",
    "\n",
    "    except Exception as e :\n",
    "        print(\"logined failed\")\n",
    "        raise e\n",
    "    \n",
    "    valid_texts = []\n",
    "    valid_docs_original = []\n",
    "\n",
    "    for doc in document:\n",
    "        try:\n",
    "            content = \"\"\n",
    "            if hasattr(doc, \"page_content\") and doc.page_content:\n",
    "                content = doc.page_content\n",
    "            elif hasattr(doc, \"metadata\") and \"original_content\" in doc.metadata:\n",
    "                content = doc.metadata['original_content']\n",
    "            \n",
    "            if content:\n",
    "                valid_texts.append(content)\n",
    "                valid_docs_original.append(doc)\n",
    "            else:\n",
    "                print(f\"Skipping document with no content: {doc.metadata.get('source', 'unknown')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from doc: {e}\")\n",
    "\n",
    "    print(f\"Extracted {len(valid_texts)} valid texts\")\n",
    "\n",
    "    if not valid_texts:\n",
    "        raise ValueError(\"No valid texts extracted from documents.\")\n",
    "    \n",
    "    embbed_model=TitanEmbeddings()\n",
    "    try:\n",
    "        sample_vec = embbed_model.invoke(valid_texts[0])\n",
    "        dim = len(sample_vec)\n",
    "        print(\"Created Dense Vectors, Dimension of each vector is: \", dim)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to generate valid embedding: {e}\")\n",
    "\n",
    "    \n",
    "    bm25 = BM25Encoder.default()\n",
    "    bm25.fit(valid_texts)\n",
    "\n",
    "    print(\"Initializing Pinecone...\")\n",
    "    pc = Pinecone(api_key=os.environ.get('PINECONE_API_KEY'))\n",
    "    existing_indexes = [i[\"name\"] for i in pc.list_indexes()]\n",
    "    print(user_id)\n",
    "    if user_id not in existing_indexes:\n",
    "        print(f\"Creating new Pinecone index: {user_id}\")\n",
    "        pc.create_index(\n",
    "            name=user_id,\n",
    "            dimension=dim,\n",
    "            metric=\"dotproduct\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Using existing Pinecone index: {user_id}\")\n",
    "\n",
    "    print(\"Created/Logined to Pinecone Successfully\")\n",
    "    index=pc.Index(user_id)\n",
    "    vectors_to_upsert=[]\n",
    "    for i, doc in enumerate(valid_docs_original):\n",
    "        doc_id = uuid.uuid4().hex[:15]\n",
    "        try:\n",
    "\n",
    "            metadata_dict = doc.metadata if isinstance(doc.metadata, dict) else {}\n",
    "            orig_cnt = json.loads(metadata_dict['original_content'])\n",
    "            \n",
    "            bucket_content = {\n",
    "                \"id\": doc_id,\n",
    "                \"raw_text\": orig_cnt.get('raw_text', \"\"),\n",
    "                \"summ_text\": valid_texts[i],\n",
    "                'table_as_html':orig_cnt.get('table_as_html', {}),\n",
    "                'image_base64':orig_cnt.get('image_base64', {})\n",
    "            }\n",
    "\n",
    "            obj.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=f\"{user_id}/{doc_id}.json\",\n",
    "            Body=json.dumps(bucket_content),\n",
    "            ContentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "\n",
    "            dense_vector = embbed_model.invoke(valid_texts[i])\n",
    "            sparse_vector = bm25.encode_documents(valid_texts[i])\n",
    "            \n",
    "            vector = {\n",
    "                \"id\": doc_id,\n",
    "                \"values\": dense_vector,\n",
    "                \"sparse_values\": sparse_vector,\n",
    "                \"metadata\": {\n",
    "                    \"text\": valid_texts[i], \n",
    "                    \"s3_uri\": f\"s3//{bucket_name}/{user_id}/{doc_id}.json\"\n",
    "    \n",
    "                }\n",
    "            }\n",
    "            vectors_to_upsert.append(vector)\n",
    "\n",
    "    \n",
    "            if len(vectors_to_upsert) >= batch_size:\n",
    "                index.upsert(vectors=vectors_to_upsert)\n",
    "                vectors_to_upsert = []\n",
    "                print(f\"Upserted batch ending at {i}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing doc {i}: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "    retriever =CustomHybridRetriever(\n",
    "        user_id=user_id,\n",
    "        emb_model=embbed_model,\n",
    "        bm25=bm25\n",
    "    )\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce71d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id=\"user-\"+str(uuid.uuid4().hex[1:12])\n",
    "user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret= upload_file(sum_chunks,user_id,BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"s3://soumya-rag/configs/084cf1909a6456.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94581271",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"what is the project about\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ret.get_relevant_documents(\n",
    "    query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5435f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3de1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "obj = s3.get_object(\n",
    "    Bucket=\"soumya-rag\",\n",
    "    Key=\"user-d959f441f15/a9f5ff528ee944b.json\"\n",
    ")\n",
    "\n",
    "data = json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94119a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,uri=docs[0].metadata['s3_uri'].split(\"//\")\n",
    "_,user_id,doc_id=uri.split('/')\n",
    "user_id,doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ea2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_document(docs) ->List[Document]:\n",
    "\n",
    "    retreived_documents=[]\n",
    "    try:\n",
    "\n",
    "        s3=boto3.client(\"s3\")\n",
    "        print(\"aws logined\")\n",
    "\n",
    "\n",
    "        for dox in docs:\n",
    "            if hasattr(dox,\"metadata\") and \"s3_uri\" in dox.metadata:\n",
    "                _,uri = dox.metadata['s3_uri'].split(\"//\")\n",
    "                _,user_id,doc_id=uri.split('/')\n",
    "\n",
    "            \n",
    "                obj=s3.get_object(\n",
    "                    Bucket=BUCKET_NAME,\n",
    "                    Key=f\"{user_id}/{doc_id}\"\n",
    "                )\n",
    "\n",
    "            data = json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "            d=Document(\n",
    "                page_content=data['raw_text'],\n",
    "                metadata={\n",
    "                    \"table_as_html\":data['table_as_html'],\n",
    "                    \"image_base64\":data['image_base64']\n",
    "                }\n",
    "            )\n",
    "\n",
    "            retreived_documents.append(d)\n",
    "\n",
    "        \n",
    "        return retreived_documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"failed due to \",e)\n",
    "        raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2171619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_docs=retreive_document(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def gen_ans(docs, query):\n",
    "    try:\n",
    "        llm = ApnaChatModel()\n",
    "\n",
    "        prompt_text = f\"\"\"\n",
    "Based on the following document context, answer the question.\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "CONTENT:\n",
    "\"\"\"\n",
    "\n",
    "        all_images = []\n",
    "\n",
    "        for i, chunk in enumerate(docs):\n",
    "            prompt_text += f\"\\n--- Document Fragment {i+1} ---\\n\"\n",
    "\n",
    "            if not hasattr(chunk, \"page_content\"):\n",
    "                continue\n",
    "\n",
    "            prompt_text += f\"TEXT:\\n{chunk.page_content}\\n\"\n",
    "\n",
    "            metadata = getattr(chunk, \"metadata\", {})\n",
    "\n",
    "            tables = metadata.get(\"table_as_html\", {})\n",
    "            if isinstance(tables, dict):\n",
    "                tables = tables.values()\n",
    "\n",
    "            if tables:\n",
    "                prompt_text += \"\\nTABLES:\\n\"\n",
    "                for j, table in enumerate(tables):\n",
    "                    prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "\n",
    "            imgs = metadata.get(\"image_base64\", [])\n",
    "            if isinstance(imgs, str):\n",
    "                imgs = [imgs]\n",
    "\n",
    "            for img in imgs:\n",
    "                decoded = base64.b64decode(img)\n",
    "                all_images.append(decoded)\n",
    "\n",
    "        prompt_text += \"\"\"\n",
    "INSTRUCTIONS:\n",
    "- Use ONLY the provided content\n",
    "- If insufficient info, say: \"I don't have enough information to answer the question.\"\n",
    "- Be concise and factual\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "        content_block = [{\n",
    "            \"text\": prompt_text\n",
    "        }]\n",
    "\n",
    "        for img_bytes in all_images:\n",
    "            content_block.append({\n",
    "                \"image\": {\n",
    "                    \"format\": \"jpeg\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": img_bytes\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "        message = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_block\n",
    "        }]\n",
    "\n",
    "        return llm.invoke(message)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Answer gen failed: {e}\")\n",
    "        raise RuntimeError(\"Sorry, I encountered an error generating the answer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67431c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ans(docs,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ai_enhanced_summary(\n",
    "    text: str,\n",
    "    tables: list,\n",
    "    images: list,\n",
    "    model: ApnaChatModel\n",
    ") -> str:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant creating a searchable description for document retrieval.\n",
    "\n",
    "--- TEXT CONTENT ---\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "    if tables:\n",
    "        prompt += \"\\n--- TABLES ---\\n\"\n",
    "        for i, table in enumerate(tables):\n",
    "            prompt += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "\n",
    "    prompt += \"\"\"\n",
    "--- YOUR TASK ---\n",
    "Generate a comprehensive, searchable description.\n",
    "\n",
    "Cover:\n",
    "1. Key facts, exact numbers, and metrics\n",
    "2. Main topics and concepts\n",
    "3. Questions this content can answer\n",
    "4. Visual insights from images (charts, diagrams, patterns)\n",
    "5. Alternative keywords and synonyms\n",
    "\n",
    "Prioritize searchability over brevity.\n",
    "\"\"\"\n",
    "\n",
    "    content_blocks = []\n",
    "\n",
    "    for img_b64 in images:\n",
    "        content_blocks.append({\n",
    "            \"image\": {\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\": {\n",
    "                    \"bytes\": img_b64.strip()\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    content_blocks.append({\n",
    "        \"text\": prompt\n",
    "    })\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_blocks\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fea2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_final_ans(chunks, query):\n",
    "    try:\n",
    "        llm = ApnaChatModel()\n",
    "\n",
    "        prompt_text = f\"\"\"\n",
    "        Based on the following document context, please answer this question: {query}\n",
    "        \n",
    "        CONTENT_TO_ANALYZE:\n",
    "        \"\"\"\n",
    "        all_images_base64 = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prompt_text += f\"\\n--- Document Fragment {i+1} ---\\n\"\n",
    "            \n",
    "            if hasattr(chunk, \"metadata\") and \"original_content\" in chunk.metadata:\n",
    "                try:\n",
    "                    original_data = json.loads(chunk.metadata['original_content'])\n",
    "                    \n",
    "                    raw_text = original_data.get(\"raw_text\", \"\")\n",
    "                    if raw_text:\n",
    "                        prompt_text += f\"Text:\\n{raw_text}\\n\\n\"\n",
    "\n",
    "                    tables_html = original_data.get(\"tables_html\", [])\n",
    "                    if tables_html:\n",
    "                        prompt_text += 'TABLES:\\n'\n",
    "                        for j, table in enumerate(tables_html):\n",
    "                            prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "                    \n",
    "                    \n",
    "                    imgs = original_data.get(\"images_base64\", [])\n",
    "                    if imgs:\n",
    "                        all_images_base64.extend(imgs)\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    prompt_text += f\"Text:\\n{chunk.page_content}\\n\\n\"\n",
    "            else:\n",
    "                prompt_text += f\"Text:\\n{chunk.page_content}\\n\\n\"\n",
    "\n",
    "        prompt_text += \"\"\" \n",
    "        INSTRUCTIONS:\n",
    "        Provide a clear, comprehensive answer using the text, tables, and images provided above. \n",
    "        If the documents don't contain sufficient information to answer the question, state: \"I don't have enough information to answer the question.\"\n",
    "        \n",
    "        ANSWER:\"\"\"\n",
    "    \n",
    "        message_content = [{'type': 'text', 'text': prompt_text}]\n",
    "\n",
    "        for img_b64 in all_images_base64:\n",
    "            message_content.append({\"type\": \"image_url\",\"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}})\n",
    "\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer gen failed: {e}\")\n",
    "        return 'Sorry, I encountered an error generating the answer.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
